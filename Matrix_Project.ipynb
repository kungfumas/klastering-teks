{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we got 100 movie titles and their information (combined from imdb and wikipedia)\n",
    "#read the movie titles\n",
    "titles = open('plot_summaries_titles.txt').read().split('\\n')\n",
    "titles = titles[:100] # ensures that only the first 100 are read in\n",
    "\n",
    "# The wiki information and imdb imforamtion of each movie is separated by the keywords \"BREAKS HERE\".\n",
    "# Each information may consist of multiple paragraphs.\n",
    "info_wiki = open('plot_summaries_wiki.txt').read().split('\\n BREAKS HERE')\n",
    "info_wiki = info_wiki[:100]\n",
    "\n",
    "info_imdb = open('plot_summaries_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "info_imdb = info_imdb[:100]\n",
    "\n",
    "# Combine imdb and wiki to get full inforamtion for the movies.\n",
    "info = []\n",
    "for i in range(len(info_wiki)):\n",
    "    item = info_wiki[i] + info_imdb[i]\n",
    "    info.append(item)\n",
    "\n",
    "# Because these movies information have already been ranked, we just need\n",
    "# to generate a list of ordered numbers for future usage.\n",
    "ranks = range(1, 1+len(titles)) # 1~100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Tokenizing and Stemming'\n",
    "Let's load stopwords and stemmer function from NLTK library. Some examples of stopwords are \"a\", \"the\", and \"in\", which don't convey siginificant meaning. Stemming is the process of breaking a word down into its plain form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk's English stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')  #nltk.corpus.\n",
    "\n",
    "print (\"We use\" + str(len(stopwords)) + \" stopwords from nltk library.\")\n",
    "print (\"Examples:\", stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenization_and_stemming(text, stemming=True):\n",
    "    tokens=[]\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if word not in stopwords:\n",
    "                tokens.append(word.lower())\n",
    "                \n",
    "    # filter out any tokens which does not contain letters (e.g., numeric tokens, raw punctuation)\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    if stemming:\n",
    "        stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        return stems\n",
    "    else:\n",
    "        return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "tokenization_and_stemming(\"Amy looked at her father's arm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our defined functions to analyze (i.e. tokenize, stem) our information\n",
    "docs_stemmed = []\n",
    "docs_tokenized = []\n",
    "for i in info:\n",
    "    tokenized_and_stemmed_results = tokenization_and_stemming(i)\n",
    "    docs_stemmed.extend(tokenized_and_stemmed_results)\n",
    "    tokenized_results = tokenization_and_stemming(i, stemming=False)\n",
    "    docs_tokenized.extend(tokenized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a mapping from stemmed words to original tokenized words for result interpretation\n",
    "vocab_frame_dict = {docs_stemmed[x]:docs_tokenized[x] for x in range(len(docs_stemmed))}\n",
    "#test\n",
    "print (vocab_frame_dict['soldier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: TF-IDF\n",
    "Transform the 100 information to a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer parameters\n",
    "tfidf_model = TfidfVectorizer(max_df=0.8, min_df=0.2, stop_words='english', \n",
    "                              use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,1))\n",
    "\n",
    "# fit the vectorizer to information\n",
    "tfidf_matrix = tfidf_model.fit_transform(info)\n",
    "\n",
    "print (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n",
    "        \" information and \" + str(tfidf_matrix.shape[1]) + \" terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the terms identified by TF_IDF\n",
    "tf_selected_words = tfidf_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: K-means clustering\n",
    "Let's use K-means to group the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 3   # arbitrary choice\n",
    "km = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse K-means Results\n",
    "# create DataFrame films from all of the input files.\n",
    "films = { 'title': titles, 'rank': ranks, 'cluster': clusters}\n",
    "frame = pd.DataFrame(films, columns = ['rank', 'title', 'cluster'])\n",
    "frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\" Number of movies included in each cluster: \")\n",
    "frame['cluster'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = frame[['rank', 'cluster']].groupby('cluster')\n",
    "print (\"Average rank (1 to 100) per cluster: \")\n",
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"<Document clustering result by K-means>\")\n",
    "\n",
    "# km.cluster_centers_ denotes the importances of each items in centroid.\n",
    "# need to sort it in descending order and get the top k items.\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "Cluster_keywords_summary = {}\n",
    "for i in range(num_clusters):\n",
    "    print (\"Cluster \" + str(i) + \" words:\") ,\n",
    "    Cluster_keywords_summary[i] = []\n",
    "    for ind in order_centroids[i, :10]:  # get the top 6 words of each cluster\n",
    "        Cluster_keywords_summary[i].append(vocab_frame_dict[tf_selected_words[ind]])\n",
    "        print (vocab_frame_dict[tf_selected_words[ind]] + \", \"),\n",
    "    print\n",
    "    \n",
    "    cluster_movies = frame.loc[frame.cluster == i, 'title'].values.tolist()\n",
    "    print (\"Cluster \" + str(i) + \" titles (\" + str(len(cluster_movies)) + \" movies): \")\n",
    "    print (\", \".join(cluster_movies), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see from the above results to find the main topics of the three cluster of movies are:\n",
    "# Families, love and life\n",
    "# Wars and battles\n",
    "# Detective and crime\n",
    "\n",
    "\n",
    "# Plot K-means Result\n",
    "# use PCA to select 2 principal components for visualization\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_matrix_np=tfidf_matrix.toarray()\n",
    "X = pca.fit_transform(tfidf_matrix_np)\n",
    "xs, ys = X[:, 0], X[:, 1]\n",
    "\n",
    "# Set up colors per clusters using a dictionary\n",
    "cluster_colors = {0: 'k', 1: 'b', 2: 'r'}\n",
    "# set up cluster names using a dictionary\n",
    "cluster_names = {}\n",
    "for i in range(num_clusters):\n",
    "    cluster_names[i] = \", \".join(Cluster_keywords_summary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# create data frame with PCA cluster results\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles))\n",
    "# groups = df.groupby(clusters)\n",
    "groups = df.groupby('label')\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "#Set color for each cluster/group\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12,\n",
    "           label=cluster_names[name], color=cluster_colors[name],\n",
    "           mec='none')\n",
    "\n",
    "#show legend with only 1 point\n",
    "ax.legend(numpoints=1, loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5: Topic Modeling - Latent Dirichlet Allocation\n",
    "Use LDA to group the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use inforamtion to build a 100*538 matrix\n",
    "terms_dict = {tf_selected_words[x]:x for x in range(len(tf_selected_words))}\n",
    "\n",
    "feature_matrix_info_tf = []\n",
    "for i in info:\n",
    "    current_doc_stats = [0] * len(tf_selected_words)\n",
    "    allwords_stemmed = tokenization_and_stemming(i)\n",
    "    for get_terms in allwords_stemmed:\n",
    "        if get_terms in tf_selected_words:\n",
    "            current_doc_stats[terms_dict[get_terms]] += 1\n",
    "    current_doc_stats = np.asarray(current_doc_stats)\n",
    "    feature_matrix_info_tf.append(current_doc_stats)\n",
    "    \n",
    "feature_matrix_info_tf = np.asarray(feature_matrix_info_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda.LDA(n_topics=3, n_iter=500, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@\"model.topic_word_\"saves the importance of tf_selected_words in LDA model, i.e. words similarity matrix.\n",
    "# Its shape is (n_toipcs, num_of_selected_words)\n",
    "#@\"model.doc_topic_\"saves the document topic results, i.e. document topic matrix. \n",
    "# Its shape is (num_of_documents, n_topics)\n",
    "\n",
    "model.fit(feature_matrix_info_tf)\n",
    "topic_word = model.topic_word_   # model.components_ also works\n",
    "n_top_words = 10\n",
    "\n",
    "topic_keywords_list = []\n",
    "for topic_dist in topic_word:\n",
    "    # Here we select the top 6 (n_top_words) words\n",
    "    lda_topic_words = np.array(tf_selected_words)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    for i in range(len(lda_topic_words)):\n",
    "        lda_topic_words[i] = vocab_frame_dict[lda_topic_words[i]]\n",
    "    topic_keywords_list.append(lda_topic_words.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "topic_doc_dict = {}\n",
    "\n",
    "print (\"<Document clustering result by LDA>\")\n",
    "for i in range(len(doc_topic)):\n",
    "    topicID = doc_topic[i].argmax()\n",
    "    if topicID not in topic_doc_dict:\n",
    "        topic_doc_dict[topicID] = [titles[i]]\n",
    "    else:\n",
    "        topic_doc_dict[topicID].append(titles[i])\n",
    "for i in topic_doc_dict:\n",
    "    print (\"Cluster \" + str(i) + \" words: \" + \", \".join(topic_keywords_list[i]))\n",
    "    print (\"Cluster \" + str(i) + \" titles (\" + str(len(topic_doc_dict[i])) + \" movies): \")\n",
    "    print (', '.join(topic_doc_dict[i]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a similar set of three clusters/topics as those we got with KMeans, but the way ther are grouped is different."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
